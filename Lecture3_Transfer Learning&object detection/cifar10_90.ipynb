{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: gpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dataset # for loading dataset (mnist)\n",
    "import torchvision.transforms as transforms # for processing datasets\n",
    "from torch.utils.data import DataLoader # for making dataset easier to use \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: gpu\") if torch.cuda.is_available() else print(\"device: cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypter parameter setting\n",
    "epochs = 200\n",
    "learning_rate = 1e-2\n",
    "display_step = 20\n",
    "batch_size = 1024\n",
    "# momentum = 0.9\n",
    "dropout_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "len(train_data):  50000\n",
      "len(test_data):  10000\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "torchvision_transform = transforms.Compose([\n",
    "    transforms.Resize((36, 36)), \n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = dataset.CIFAR10(\"./\", train = True, transform = torchvision_transform, target_transform = None, download = True)\n",
    "test_data = dataset.CIFAR10(\"./\", train = False, transform = transforms.ToTensor(), target_transform = None, download = True)\n",
    "\n",
    "# check the data\n",
    "print('len(train_data): ', len(train_data))\n",
    "print('len(test_data): ', len(test_data))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() # for initializing nn.Module (parent class)\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(128, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "     \n",
    "    def forward(self, x):\n",
    "        output = self.feature_extraction(x)\n",
    "\n",
    "        flatten = output.view(batch_size, -1)\n",
    "        result = self.classifier(flatten)\n",
    "        return result\n",
    "\n",
    "model = CNN().to(device)\n",
    "model.train()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch loss: 1.8045344352722168\n",
      "Accuracy of the model: 0.1955295205116272\n",
      "20 epoch loss: 0.32153743505477905\n",
      "Accuracy of the model: 0.800889790058136\n",
      "40 epoch loss: 0.1851670742034912\n",
      "Accuracy of the model: 0.8539496660232544\n",
      "60 epoch loss: 0.13235488533973694\n",
      "Accuracy of the model: 0.8645833134651184\n",
      "80 epoch loss: 0.047368984669446945\n",
      "Accuracy of the model: 0.8814018964767456\n",
      "100 epoch loss: 0.012992381118237972\n",
      "Accuracy of the model: 0.88671875\n",
      "120 epoch loss: 0.012830565683543682\n",
      "Accuracy of the model: 0.8908420205116272\n",
      "140 epoch loss: 0.008919920772314072\n",
      "Accuracy of the model: 0.8865017294883728\n",
      "160 epoch loss: 0.01624266244471073\n",
      "Accuracy of the model: 0.8909505009651184\n",
      "180 epoch loss: 0.009657550603151321\n",
      "Accuracy of the model: 0.8986892294883728\n"
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    lr = learning_rate * (0.1 ** (i // 100))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        loss_array.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        #test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for index, [data, label] in enumerate(test_loader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                \n",
    "                output = model.forward(data)\n",
    "                _, prediction_index = torch.max(output, 1)\n",
    "                \n",
    "                total += label.size(0)\n",
    "                correct += (prediction_index == label).sum().float()\n",
    "\n",
    "            if total == 0:\n",
    "                total = 1\n",
    "            print(\"Accuracy of the model: {}\".format(correct/total))\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.9016102695465088\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, [data, label] in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = model.forward(data)\n",
    "        _, prediction_index = torch.max(output, 1)\n",
    "\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (prediction_index == label).sum().float()\n",
    "\n",
    "    print(\"Accuracy of the model: {}\".format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모멘텀 0.9일때 52에포스에 59%\n",
    "# 모멘텀 0.8로 수정 후 재시작 36에포스에 65%가 최고기록\n",
    "# 드롭아웃 제거, 데이터 증강법 적용 54에포스에 68% 기록\n",
    "# 매 에포스마다 데이터 증강법 적용되도록 수정 16에포스에 67% 기록\n",
    "# 모멘텀 0.7로 수정 16에포스에 61%기록 다시 0.8로 복구\n",
    "# 러닝레이트 1e-4로 수정, 데이터증강법 시도한거 다 삭제. 잘못되어 있엇음\n",
    "# cnn모델 각 스테이지별 블록 개수 2->3으로 증가 57%가 끝\n",
    "# learning_rate = 1e-1, momentum=0.9, weight_decay=5e-4로 변경 각스테이지별 블록 개수 3->2로 복구   22에 62달성 일단 중단\n",
    "# 학습주기별 러닝레이트 변경 기능 추가 learning_rate_plan = [[50, 1e-1], [50, 1e-2], [50, 1e-3]] 로 시도. 첫단계에서 아에 안내려감\n",
    "# learning_rate_plan = [[50, 1e-2], [50, 1e-3], [50, 1e-4]]로 시도 70%가 끝\n",
    "# 배치 사이즈 128에서 32로 축소, 아주간단한 모델로 변경 -> 79% 달성\n",
    "# 배치사이즈 그대로, 원래 복잡한 모델로 변경 14->78%,  82%달성\n",
    "# 간단한 모델로 변경, 소프트 맥스 추가, 아담으로 변경, learning_rate_plan = [[20, 1e-4], [20, 1e-5], [20, 1e-6], [20, 1e-7]] 84%\n",
    "# 드롭아웃 2개중 1개꼴로 추가 83%\n",
    "# 드롭아웃 전부다 추가, 드롭아웃에 relu실수로 들어가있던거 다 삭제 10에 80.3% 중단\n",
    "# 64 128에 한층씩 축소 6에 79.3% 줄였지만 큰 차이 없음. 중단\n",
    "# 소프트맥스에도 dropout추가, 512 하나 추가 12에 81.9%\n",
    "# 복잡한 모델  8에 74% 탈락\n",
    "# 간단한 모델 더 빠른 학습을 위해 learning_rate_plan = [[10, 1e-3], [10, 1e-4], [20, 1e-5]] 로 변경 20에 85%로 중단\n",
    "# 10, 1e-3에서 10은 너무 짧았던것 같음. 10에 82.2%로끝났는데 로스는 이전까지 절반으로 줄어두는 중이었음\n",
    "# learning_rate_plan = [[20, 1e-3], [20, 1e-4], [20, 1e-5]] 로 적용, 끝에 512->256 추가 14에 82.5%\n",
    "# 256 -> 128 추가 8에 82.9%\n",
    "# 128 -> 64 추가 24에 84.7%\n",
    "# learning_rate_plan = [[20, 1e-1], [20, 1e-2], [20, 1e-3], [20, 1e-4], [20, 1e-5]] 로 변경, 소프트 맥스 제거, eval호출 후 test데이터 측정 후 train호출 추가\n",
    "# 러닝레이트 적용 제대로 된걸로 수정 후 복잡한 모델. \n",
    "# 복잡한 모델 삭제. 비교상대가 안된다.\n",
    "# 배치 사이즈 줄인다고 성능 향상이 되는것은 아니다. 빨라진만큼 러닝레이트를 늘려주면 정확도는 비슷한데 속도는 더 빠르다.\n",
    "# 맥스풀링간의 간격이 너무 좁으면 안된다.\n",
    "# 맥스풀링간의 간격을 2개에서 3개로 늘림. 74에서 89.69% 달성.\n",
    "# 끝에 3->16, 64->32 추가\n",
    "# 로스가 0.0x인 것이 0.0000x인 것보다 좋기도 하다. 로스가 적을수록 또는 어느정도 이하일때 항상 좋은게 아니다.\n",
    "# 3->16, 64->32 삭제 에포스 단계별 100으로 증가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
